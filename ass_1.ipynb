{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class tf_idf_search_engine():\n",
    "    def __init__(self, folder_path):\n",
    "        self.path = folder_path\n",
    "        self.file_cnt = len(os.listdir(folder_path + \"/data\"))\n",
    "        self.overlapping_doc_cnt = -1\n",
    "        self.data_lst = list()\n",
    "        self.data_split_lst = list() \n",
    "        \n",
    "        #PATH initialize\n",
    "        for i in range(self.file_cnt):\n",
    "            data_path = path + \"/data/\" + str(i) + \".txt\"\n",
    "            data = open(data_path, encoding='utf8').readline()\n",
    "            self.data_lst.append(data)  \n",
    "            \n",
    "        #for stemming\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.english_stops = list(set(stopwords.words('english')))\n",
    "        \n",
    "        #delete special-char at stop words, special-char and stemming\n",
    "        for idx,stop_word in enumerate(self.english_stops):\n",
    "            after_word = \"\"\n",
    "            stop_word = self.stemmer.stem(stop_word)\n",
    "            for c in stop_word:\n",
    "                if c.isalnum():\n",
    "                    after_word += c\n",
    "            if stop_word == after_word:\n",
    "                pass\n",
    "            else:\n",
    "                self.english_stops[idx] = after_word\n",
    "                \n",
    "        #query\n",
    "        self.query_path = path + \"/query.txt\" \n",
    "        self.query = open(self.query_path, encoding='utf8').readline()\n",
    "        self.query_lst = self.query.split()\n",
    "        \n",
    "        #stemming query, if not stemming , can't catch 'president --> presidi' \n",
    "        for idx,query in enumerate(self.query_lst):\n",
    "            self.query_lst[idx] = self.stemmer.stem(query)\n",
    "            \n",
    "        #TF_lst , IDF_lst , TF_IDF\n",
    "        self.TF_lst = list() \n",
    "        self.inverted_idx_dict=dict()\n",
    "        self.tf_idf = dict()\n",
    "        self.overlapping_doc_lst = list()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        #all char get lower\n",
    "        for i in range(self.file_cnt):\n",
    "            self.data_lst[i] = self.data_lst[i].lower()\n",
    "            self.data_split_lst.append(self.data_lst[i].split())\n",
    "            #delete all \"\"(cnn)\"\"  mark at all doc\n",
    "            if \"(cnn)\" in self.data_split_lst[i][0]:\n",
    "                self.data_split_lst[i][0] = self.data_split_lst[i][0].replace(\"(cnn)\",\"\")\n",
    "                \n",
    "            #delete special-char\n",
    "            for idx,word in enumerate(self.data_split_lst[i]):\n",
    "                after_word = \"\"\n",
    "                for c in word:\n",
    "                    if c.isalnum():\n",
    "                        after_word += c     \n",
    "                if word == after_word:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.data_split_lst[i][idx] = after_word\n",
    "                    \n",
    "            #stemming    \n",
    "            for idx, original_word in enumerate(self.data_split_lst[i]):\n",
    "                if original_word == '':\n",
    "                    self.data_split_lst[i].remove(original_word)\n",
    "                else:\n",
    "                    changed_word = self.stemmer.stem(original_word)\n",
    "                    if original_word == changed_word:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.data_split_lst[i][idx] = changed_word       \n",
    "                        \n",
    "            #delete stopwords\n",
    "            for word in self.data_split_lst[i]:\n",
    "                if word in self.english_stops:\n",
    "                    while word in self.data_split_lst[i]:\n",
    "                        self.data_split_lst[i].remove(word)\n",
    "                        \n",
    "    #get doc with query\n",
    "    def get_overlapping_doc(self):\n",
    "        query_len = len(self.query_lst)\n",
    "        if query_len != 0:\n",
    "            overlapping_doc_lst = list()\n",
    "            query_set = set(self.query_lst)\n",
    "            for idx, split_doc in enumerate(self.data_split_lst):\n",
    "                split_doc_set = set(split_doc)\n",
    "                intersection_cnt = len(list(query_set.intersection(split_doc_set)))\n",
    "                if intersection_cnt == query_len:\n",
    "                    doc_no = \"doc\" + str(idx)\n",
    "                    overlapping_doc_lst.append(doc_no)\n",
    "                else:\n",
    "                    pass\n",
    "            self.overlapping_doc_lst = overlapping_doc_lst\n",
    "            self.overlapping_doc_cnt = len(overlapping_doc_lst)\n",
    "        else:\n",
    "            print(\"There is no Query!\")\n",
    "        \n",
    "    def calculating_TF_lst(self):\n",
    "        for i in range(self.file_cnt):\n",
    "            word_cnt=dict()\n",
    "            for word in self.data_split_lst[i]:\n",
    "                if word not in word_cnt:\n",
    "                    word_cnt[word]=1\n",
    "                else:\n",
    "                    word_cnt[word]= word_cnt[word]+1  \n",
    "            self.TF_lst.append(word_cnt) \n",
    "            \n",
    "    def getting_TF(self, doc_no, term):\n",
    "        if term in self.TF_lst[doc_no]:\n",
    "            TF = math.log10(1+self.TF_lst[doc_no][term])\n",
    "        else:\n",
    "            TF = 0.0\n",
    "        return TF\n",
    "    \n",
    "    def making_inverted_idx(self):\n",
    "        for i in range(len(self.TF_lst)): \n",
    "            for key in self.TF_lst[i].keys():\n",
    "                # key(term) not in IDF_dict\n",
    "                if key not in self.inverted_idx_dict.keys():\n",
    "                    doc_num = \"doc\" + str(i)\n",
    "                    self.inverted_idx_dict[key]=[doc_num]  \n",
    "                # key(term) in IDF_dict, only add doc_num\n",
    "                elif key in self.inverted_idx_dict.keys() and key not in self.inverted_idx_dict[key]:\n",
    "                    doc_num = \"doc\" + str(i)\n",
    "                    self.inverted_idx_dict[key].append(doc_num) \n",
    "                    \n",
    "    def getting_IDF(self, term):\n",
    "        IDF = math.log10(float(self.file_cnt)/len(self.inverted_idx_dict[term]))\n",
    "        return IDF\n",
    "    \n",
    "    def getting_weight(self, doc_no, term):\n",
    "        return self.getting_TF(doc_no, term) * self.getting_IDF(term)  \n",
    "    \n",
    "    def building_tf_idf(self):\n",
    "        for i in range(self.file_cnt):\n",
    "            w_dict = dict()\n",
    "            for key in self.inverted_idx_dict.keys():\n",
    "                w_dict[key] = self.getting_weight(i, key)   \n",
    "            doc_no = \"doc\" + str(i)\n",
    "            self.tf_idf[doc_no] = w_dict \n",
    "            \n",
    "        #make tf-idf for query\n",
    "        w_dict = dict()\n",
    "        for key in self.inverted_idx_dict.keys():\n",
    "            w_dict[key] = 0.0\n",
    "        for query in self.query_lst:\n",
    "            TF = math.log10(1+1)\n",
    "            IDF = math.log10(self.file_cnt/len(self.inverted_idx_dict[query]))\n",
    "            w_dict[query] =  TF * IDF\n",
    "        self.tf_idf['query'] = w_dict\n",
    "        \n",
    "    #get vector size\n",
    "    def get_size(self,vector):\n",
    "        size = len(vector)\n",
    "        sum = 0.0\n",
    "        for v in vector:\n",
    "            sum += math.pow(v,2)\n",
    "        result = math.sqrt(sum)\n",
    "        return result\n",
    "    \n",
    "    #get vector innerproduct\n",
    "    def get_innerproduct(self,v1, v2):\n",
    "        size = len(v1)\n",
    "        result = 0.0\n",
    "        for a,b in zip(v1,v2):\n",
    "            result += a*b\n",
    "        return result\n",
    "    \n",
    "    def cosine_similarity(self, v1, v2):\n",
    "        return self.get_innerproduct(v1,v2) / (self.get_size(v1) * self.get_size(v2))\n",
    "    \n",
    "    #get doc with highest score\n",
    "    def get_score(self):\n",
    "        top_sum = -100.0\n",
    "        top_no = \"doc\"  \n",
    "        score_dict =dict()\n",
    "        query_vector = list(self.tf_idf['query'].values())\n",
    "        \n",
    "        for doc_no in self.overlapping_doc_lst:\n",
    "            doc_vector = list(self.tf_idf[doc_no].values())\n",
    "            score = self.cosine_similarity(query_vector, doc_vector)\n",
    "            score_dict[doc_no] = score \n",
    "            if top_sum < score :\n",
    "                top_sum = score\n",
    "                top_no = doc_no\n",
    "        \n",
    "        score_tuple = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)          \n",
    "        return top_sum, top_no , score_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\" #your path\n",
    "en = tf_idf_search_engine(path)\n",
    "en.preprocess_data()\n",
    "en.get_overlapping_doc()\n",
    "en.calculating_TF_lst()\n",
    "en.making_inverted_idx()\n",
    "en.building_tf_idf()\n",
    "en.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
